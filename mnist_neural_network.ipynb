{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#The action plan on MNIST dataset\n",
        "\n",
        "### 1. Prepare our data and preprocess it.\n",
        "### 2. Create training, validation and test dataset\n",
        "### 3. Outline the model and choose the activation functions\n",
        "### 4. Set the appropriate advanced optimizers and loss funtions\n",
        "### 5. Make it learn\n",
        "### 6. Test the accuracy of the model"
      ],
      "metadata": {
        "id": "W4W7FvtAiewJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the relevant packages"
      ],
      "metadata": {
        "id": "cUEsy81eng3_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "i4SBc5dFg0b6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Storing the data as the mnist_dataset variable and setting the arguments ***as_supervised*** and ***with_info*** to **True** so that the data loads into two tuple structure **[input, target]** and provides a tuple containing info about version, features, number of samples of the dataset."
      ],
      "metadata": {
        "id": "Kb1cqClkoJJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_dataset, mnist_info = tfdf.load(name='mnist', with_info=True, as_supervised=True)\n"
      ],
      "metadata": {
        "id": "PkQPf4e24qh6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare training, validation and test datasets and preprocess it."
      ],
      "metadata": {
        "id": "E_6jDNpN-CDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
        "\n",
        "num_validation_samples = 0.1*(mnist_info.splits['train'].num_examples)\n",
        "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
        "\n",
        "num_test_samples = mnist_info.splits['test'].num_examples\n",
        "num_test_samples = tf.cast(num_test_samples, tf.int64)\n"
      ],
      "metadata": {
        "id": "KT6lDTO29-Ij"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scaling the data (inputs between 0 and 1) by defining a function named **scale**.\n",
        "### since, MNIST images contain values from 0 to 255 on the grey scale therefore if we devide each element by 255, we will get the desired result."
      ],
      "metadata": {
        "id": "6AjkUHL2BVTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scale(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image /= 255.\n",
        "  return image, label\n"
      ],
      "metadata": {
        "id": "Dkk9JXV0-9yO"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_train_and_validation_data = mnist_train.map(scale)\n",
        "\n",
        "test_data = mnist_test.map(scale)"
      ],
      "metadata": {
        "id": "KiY65HSV91PZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Since we will be batching , we'de better shuffle the data. it should be as randomly spread as possible so that the batching works as intended.\n",
        "##here, 1 < BUFFER_SIZE < num_samples"
      ],
      "metadata": {
        "id": "8Zxzn_8nHts9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 1000\n",
        "\n",
        "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
        "\n",
        "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
        "\n",
        "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
        "\n"
      ],
      "metadata": {
        "id": "muDN0U2r_u_D"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I will be using Minibatch gradient descent to the model.\n",
        "## 1 < BATCH_SIZE < number of samples = mini-batch GD\n",
        "### Note: Model expects our validation set in batch form too. we choose the batch size for validation set equal to the total number of validation samples so we will have a single batch for this.\n",
        "### since, during validating and testing we want the exact values.  "
      ],
      "metadata": {
        "id": "jkTfHGxOK9tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 100\n",
        "\n",
        "train_data = train_data.batch(BATCH_SIZE)\n",
        "\n",
        "validation_data = validation_data.batch(num_validation_samples)\n",
        "\n",
        "test_data = test_data.batch(num_test_samples)"
      ],
      "metadata": {
        "id": "EuRFK7qYA3js"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Our data is iterable and in 2-tuple format (as_supervised=True)\n",
        "###therefore, first extract and convert the validation inputs and targets appropriately."
      ],
      "metadata": {
        "id": "A0gAtLiaQ0yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_inputs, validation_targets = next(iter(validation_data))"
      ],
      "metadata": {
        "id": "2k8pGPquCPRO"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Outline the model and choose the activation functions\n",
        "## Since, our data is (from tfdf) is such that each input is 28 * 28 * 1 or a tensor of rank 3 so we need to flatten the images(tensor) into a vector.\n",
        "##"
      ],
      "metadata": {
        "id": "Lnle5wmuTB5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_size = 784\n",
        "output_size = 10\n",
        "hidden_layer_size = 100\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
        "                            ])\n"
      ],
      "metadata": {
        "id": "FB4aMETaCvMX"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choosing Optimizer and the Loss function\n",
        "##Here, model.compile(optimizer, loss , metrics) configures the model for training."
      ],
      "metadata": {
        "id": "-XZtJu7aXy2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "0gy0-MYsFCJq",
        "collapsed": true
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training the model"
      ],
      "metadata": {
        "id": "cCnFG96pdvJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 5\n",
        "\n",
        "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Ip-1I4eqFuxZ",
        "outputId": "bb74d365-1d7a-47b7-e6b8-d8bd57f4b775"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "540/540 - 7s - 14ms/step - accuracy: 0.9071 - loss: 0.3199 - val_accuracy: 0.9530 - val_loss: 0.1567\n",
            "Epoch 2/5\n",
            "540/540 - 6s - 11ms/step - accuracy: 0.9616 - loss: 0.1265 - val_accuracy: 0.9653 - val_loss: 0.1165\n",
            "Epoch 3/5\n",
            "540/540 - 6s - 11ms/step - accuracy: 0.9728 - loss: 0.0893 - val_accuracy: 0.9697 - val_loss: 0.1067\n",
            "Epoch 4/5\n",
            "540/540 - 10s - 18ms/step - accuracy: 0.9801 - loss: 0.0656 - val_accuracy: 0.9732 - val_loss: 0.0935\n",
            "Epoch 5/5\n",
            "540/540 - 10s - 18ms/step - accuracy: 0.9829 - loss: 0.0555 - val_accuracy: 0.9737 - val_loss: 0.0938\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f8c797c6ae0>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##We usually look at the validation loss (or set early stopping mechanisms) to determine whether the model is overfitting.\n",
        "##Validation_accuracy = True accuracy of the model for the epoch\n",
        "##Since, training accuracy is the average accuracy across batches while the validation accuracy is that of the whole validation set.\n",
        "\n",
        "#To assess the averall accuracy of our model we look at the validation accuracy for the last epoch."
      ],
      "metadata": {
        "id": "gHaC-zMWccG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train:-     (This make our model to learn and optimizes the model)\n",
        "\n",
        "#Validate:-    (This make sure our parameters- the weights and biases- don't overfit)\n",
        "\n",
        "#Test:-    (This make sure our Hyperparameters- width, depth, batch size, number of epochs, etc.- don't overfit)"
      ],
      "metadata": {
        "id": "aG5UDmXFkp0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy, test_loss = model.evaluate(test_data)            #model.evaluate returns the loss value and matrics values for the model in the test mode\n",
        "print('Test accuracy: {0: .2f}% Test loss: {1: .2f}%'.format(test_accuracy*100, test_loss*100.))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "owQTMhS5l8y2",
        "outputId": "7256820f-2d72-4eb3-881d-b1e3207bdc8a"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 830ms/step - accuracy: 0.9734 - loss: 0.0845\n",
            "Test accuracy:  8.45% Test loss:  97.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Getting a test accuracy very close to the validation accuracy shows that we have not overfit over model\n",
        "#Therefore, the model works with 97.34% accuracy in real life deployment."
      ],
      "metadata": {
        "id": "zvW4mT3kqfV4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QVCAqoxPX51n"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CjTNHCrEOTCq"
      },
      "execution_count": 52,
      "outputs": []
    }
  ]
}